diff --git a/src/triped_sim/envs/triped_sim_env.py b/src/triped_sim/envs/triped_sim_env.py
index 1c6b4bc..377c1f8 100644
--- a/src/triped_sim/envs/triped_sim_env.py
+++ b/src/triped_sim/envs/triped_sim_env.py
@@ -59,7 +59,8 @@ class TripedSimSimpleEnv(gymnasium.Env):
       rot_matrix = p.getMatrixFromQuaternion(self.q)
       discount = rot_matrix[-1]  # discount is dotting the frame z-axis with the world z-axis
       if discount < 0:
-        return discount*abs(self.xyz[0])
+        # the expected height is around 0.1m
+        return discount*abs(self.xyz[0])*(self.xyz[2]/0.1)
       return discount*self.xyz[0]  # the further on x-axis the better
 
   def _get_observation(self):
diff --git a/train_dir/example_gym_triped_sim-v0/checkpoint_p0/best_000019544_10006528_reward_-10330.609.pth b/train_dir/example_gym_triped_sim-v0/checkpoint_p0/best_000019544_10006528_reward_-10330.609.pth
deleted file mode 100644
index ce6dced..0000000
Binary files a/train_dir/example_gym_triped_sim-v0/checkpoint_p0/best_000019544_10006528_reward_-10330.609.pth and /dev/null differ
diff --git a/train_dir/example_gym_triped_sim-v0/checkpoint_p0/checkpoint_000017728_9076736.pth b/train_dir/example_gym_triped_sim-v0/checkpoint_p0/checkpoint_000017728_9076736.pth
deleted file mode 100644
index 170af92..0000000
Binary files a/train_dir/example_gym_triped_sim-v0/checkpoint_p0/checkpoint_000017728_9076736.pth and /dev/null differ
diff --git a/train_dir/example_gym_triped_sim-v0/checkpoint_p0/checkpoint_000019544_10006528.pth b/train_dir/example_gym_triped_sim-v0/checkpoint_p0/checkpoint_000019544_10006528.pth
deleted file mode 100644
index 1e40072..0000000
Binary files a/train_dir/example_gym_triped_sim-v0/checkpoint_p0/checkpoint_000019544_10006528.pth and /dev/null differ
diff --git a/train_dir/example_gym_triped_sim-v0/config.json b/train_dir/example_gym_triped_sim-v0/config.json
index b58e2d6..a5225c7 100644
--- a/train_dir/example_gym_triped_sim-v0/config.json
+++ b/train_dir/example_gym_triped_sim-v0/config.json
@@ -65,8 +65,8 @@
   "summaries_use_frameskip": true,
   "heartbeat_interval": 20,
   "heartbeat_reporting_interval": 180,
-  "train_for_env_steps": 10000000,
-  "train_for_seconds": 10000000000,
+  "train_for_env_steps": 100000000,
+  "train_for_seconds": 100000000000,
   "save_every_sec": 100,
   "keep_checkpoints": 2,
   "load_checkpoint_kind": "latest",
diff --git a/train_dir/example_gym_triped_sim-v0/git.diff b/train_dir/example_gym_triped_sim-v0/git.diff
index 44c2f1a..e69de29 100644
--- a/train_dir/example_gym_triped_sim-v0/git.diff
+++ b/train_dir/example_gym_triped_sim-v0/git.diff
@@ -1,193 +0,0 @@
-diff --git a/.gitignore b/.gitignore
-index a326fca..f506dfa 100644
---- a/.gitignore
-+++ b/.gitignore
-@@ -1,3 +1,5 @@
- .vscode/
- .env/
- **/.DS_Store
-+sample-factory/
-+
-diff --git a/readme.md b/readme.md
-index 0922563..3142fb1 100644
---- a/readme.md
-+++ b/readme.md
-@@ -31,3 +31,23 @@ After removing all the collision bodies other than the feet, the simulation ran
- After reducing the complexity of the feet, the simulation ran 208fps???
- 
- Seems to be within run to run variation. I guess the simplification is not worth it.
-+
-+## Observation, Action, Reward
-+
-+### Observation
-+
-+In the spirit of making it easy to transfer the algorithm to real life, the observations will only consist of things that will be easy to obtain in real life. For now, the observations will consist of the angle and agular speed of the base_link in quarternion as well as the z-height of the base_link.
-+
-+### Action
-+
-+The action will be the desired angle of the 9 servos.
-+
-+### Reward
-+
-+This was a tricky one to design. For now I will just want the thing to follow the x-axis. There are three things to take into consideration:
-+
-+1. Higher the x value the better
-+2. Cannot go off the x-axis
-+3. Cannot tilt too much.
-+
-+Keeping these in mind, when the robot is at least the +-0.1m apart from the x-axis, the reward will be the negative y value. When the robot is within, it will be the x value times the normal vector of the base_link dotted with the z-axis. This means the further the robot walks the more reward it gets but it must also stay upright.
-diff --git a/src/gym_test.py b/src/gym_test.py
-index d0b40e0..d2278f5 100644
---- a/src/gym_test.py
-+++ b/src/gym_test.py
-@@ -8,7 +8,10 @@ info = env.reset()
- print(env.observation_space)
- print(env.action_space)
- 
--while True:
--  info = env.step(np.zeros(9))
--  print(info)
--  time.sleep(1)
-+start_time = time.time()
-+for _ in range(10000):
-+  # we are gonna do a position controller
-+  info = env.step([np.sin(time.time()/10)]*9)
-+  # print(info)
-+  # time.sleep(0.01)
-+print(10000/(time.time()-start_time))
-diff --git a/src/pybullet_test.py b/src/pybullet_test.py
-index c93702f..75c05cb 100644
---- a/src/pybullet_test.py
-+++ b/src/pybullet_test.py
-@@ -22,21 +22,22 @@ print(p.getBasePositionAndOrientation(triped))
- revolute_indices = [3, 8, 13, 5, 10, 15, 7, 12, 17]
- 
- start_time = time.time()
--for _ in range(1000):
--  # for i, joint_index in enumerate(revolute_indices):
--  #   try:
--  #     angle = p.readUserDebugParameter(angles_sliders[i])
--  #     p.setJointMotorControl2(triped, joint_index, p.POSITION_CONTROL, targetPosition=angle)
--  #   except:
--  #     print("Error reading user input")
--  # try:
--  #   finish = p.readUserDebugParameter(finish_button)
--  #   if finish:
--  #     p.resetSimulation()
--  #     plane = p.loadURDF('plane.urdf')
--  #     triped = p.loadURDF('../drawings/Triped_description/urdf/Triped.xacro', [0, 0, 0.1])
--  # except:
--  #   pass
-+for _ in range(10000):
-+  angles = []
-+  try:
-+    for i, joint_index in enumerate(revolute_indices):
-+      angle = p.readUserDebugParameter(angles_sliders[i])
-+      angles.append(angle)
-+    p.setJointMotorControlArray(triped, revolute_indices, p.POSITION_CONTROL, targetPositions=angles)
-+    
-+    finish = p.readUserDebugParameter(finish_button)
-+    if finish:
-+      p.resetSimulation()
-+      plane = p.loadURDF('plane.urdf')
-+      triped = p.loadURDF('../drawings/Triped_description/urdf/Triped.xacro', [0, 0, 0.1])
-+  except:
-+    print("Error reading user input")
-+
-   p.stepSimulation()
--print(1000/(time.time()-start_time))
-+print(10000/(time.time()-start_time))
- p.disconnect()
-diff --git a/src/triped_sim/envs/triped_sim_env.py b/src/triped_sim/envs/triped_sim_env.py
-index 519d4dd..ed33cd0 100644
---- a/src/triped_sim/envs/triped_sim_env.py
-+++ b/src/triped_sim/envs/triped_sim_env.py
-@@ -11,44 +11,77 @@ class TripedSimSimpleEnv(gymnasium.Env):
- 
-     output = p.GUI if render_mode == "human" else p.DIRECT
-     self.physicsClient = p.connect(output)
--    p.setGravity(0, 0, -10)
-     p.setAdditionalSearchPath(pybullet_data.getDataPath())  # this adds urdf paths
- 
-     self.revolute_indices = [3, 8, 13, 5, 10, 15, 7, 12, 17]
- 
-+    self.xyz = None
-+    self.q = None
-+    self.omega = None
-+
-+    self.frames = 0
-+    self.frame_limit = 10000
-+
-   def step(self, action):
--    for i, joint_index in enumerate(self.revolute_indices):
--      p.setJointMotorControl2(self.triped, joint_index, p.POSITION_CONTROL, targetPosition=action[i])
-+    # print("action: ", action)
-+    self.frames += 1
-+    p.setJointMotorControlArray(self.triped, self.revolute_indices, p.POSITION_CONTROL, targetPositions=action)
-     p.stepSimulation()
-     observation = self._get_observation()
-     reward = self._get_reward()
--    return observation, reward, False, False, {}
-+    return observation, reward, self.frames>self.frame_limit, False, {}
- 
-   def _load_models(self):
-     self.plane = p.loadURDF("plane.urdf")
-     current_dir = os.path.dirname(os.path.realpath(__file__))
-     urdf_path = os.path.join(current_dir, "../../../drawings/Triped_description/urdf/Triped.xacro")
-     self.triped = p.loadURDF(urdf_path, [0, 0, 0.1])  # drop the model at z height 0.1m
-+    p.setGravity(0, 0, -10)
- 
--  def reset(self):
-+  def reset(self, seed=None, options=None):
-+    print("reset called")
-+    self.frames = 0
-     p.resetSimulation()
-     self._load_models()
-+    p.stepSimulation()
-+    return self._get_observation(), {}
- 
-   def render(self):
-     pass
- 
-   def _get_reward(self):
--    return 0
-+    """ must have called _get_observation before calling this function """
-+    # first check if we are within +-0.1m of the x-axis
-+    if abs(self.xyz[1]) > 0.1:
-+      return -abs(self.xyz[1])
-+    else:
-+      # get the z-axis of the baselink w.r.t. world
-+      # to do so, we first the get the rotation matrix of the baselink w.r.t. world
-+      rot_matrix = p.getMatrixFromQuaternion(self.q)
-+      discount = rot_matrix[-1]  # discount is dotting the frame z-axis with the world z-axis
-+      if discount < 0:
-+        return discount*abs(self.xyz[0])
-+      return discount*self.xyz[0]  # the further on x-axis the better
- 
-   def _get_observation(self):
--    (xyz, q) = p.getBasePositionAndOrientation(self.triped)
--    obs = list(p.getMatrixFromQuaternion(q))
--    obs.append(xyz[-1])
-+    (self.xyz, self.q) = p.getBasePositionAndOrientation(self.triped)
-+    self.omega = p.getBaseVelocity(self.triped)[1]
-+    obs = list(self.q)  # start with the orientation
-+    obs += list(self.omega)
-+    obs.append(self.xyz[-1])  # add the height
-+    # # get the joint angles for all 9 revolute joints
-+    # for joint_index in self.revolute_indices:
-+    #   joint_state = p.getJointState(self.triped, joint_index)
-+    #   obs.append(joint_state[0])
-+    # print("obs: ", obs)
-     return np.array(obs)
- 
-   def _observation_spec(self):
--    # we have the rotation matrix of the baselink w.r.t. word and its height
--    return gymnasium.spaces.Box(-1, 1, shape=(10,))
-+    """
-+    we have the quaternion of the baselink w.r.t. word,
-+    the height of the baselink w.r.t. word, and
-+    """
-+    return gymnasium.spaces.Box(-1, 1, shape=(8,))
- 
-   def _action_spec(self):
-     # we have 9 revolute joints
diff --git a/train_dir/example_gym_triped_sim-v0/sf_log.txt b/train_dir/example_gym_triped_sim-v0/sf_log.txt
index 4754917..eedbf52 100644
--- a/train_dir/example_gym_triped_sim-v0/sf_log.txt
+++ b/train_dir/example_gym_triped_sim-v0/sf_log.txt
@@ -2055,3 +2055,422 @@ save_policy_outputs: 47.8100
 [2023-08-29 14:10:16,277][02257] Runner profile tree view:
 main_loop: 552.3791
 [2023-08-29 14:10:16,277][02257] Collected {0: 10006528}, FPS: 17433.1
+[2023-08-31 21:31:15,941][25909] Saving configuration to /Users/xinsonglin/Triped/train_dir/./example_gym_triped_sim-v0/config.json...
+[2023-08-31 21:31:15,964][25909] Rollout worker 0 uses device cpu
+[2023-08-31 21:31:15,964][25909] Rollout worker 1 uses device cpu
+[2023-08-31 21:31:15,964][25909] Rollout worker 2 uses device cpu
+[2023-08-31 21:31:15,964][25909] Rollout worker 3 uses device cpu
+[2023-08-31 21:31:15,964][25909] Rollout worker 4 uses device cpu
+[2023-08-31 21:31:15,964][25909] Rollout worker 5 uses device cpu
+[2023-08-31 21:31:15,965][25909] Rollout worker 6 uses device cpu
+[2023-08-31 21:31:15,965][25909] Rollout worker 7 uses device cpu
+[2023-08-31 21:31:15,965][25909] In synchronous mode, we only accumulate one batch. Setting num_batches_to_accumulate to 1
+[2023-08-31 21:31:16,185][25909] InferenceWorker_p0-w0: min num requests: 2
+[2023-08-31 21:31:16,222][25909] Starting all processes...
+[2023-08-31 21:31:16,222][25909] Starting process learner_proc0
+[2023-08-31 21:31:16,285][25909] Starting all processes...
+[2023-08-31 21:31:16,369][25909] Starting process inference_proc0-0
+[2023-08-31 21:31:16,369][25909] Starting process rollout_proc0
+[2023-08-31 21:31:16,369][25909] Starting process rollout_proc1
+[2023-08-31 21:31:16,370][25909] Starting process rollout_proc2
+[2023-08-31 21:31:16,370][25909] Starting process rollout_proc3
+[2023-08-31 21:31:16,370][25909] Starting process rollout_proc4
+[2023-08-31 21:31:16,370][25909] Starting process rollout_proc5
+[2023-08-31 21:31:16,373][25909] Starting process rollout_proc6
+[2023-08-31 21:31:16,373][25909] Starting process rollout_proc7
+[2023-08-31 21:31:18,961][25923] Starting seed is not provided
+[2023-08-31 21:31:18,962][25923] Initializing actor-critic model on device cpu
+[2023-08-31 21:31:18,962][25923] RunningMeanStd input shape: (8,)
+[2023-08-31 21:31:18,963][25923] RunningMeanStd input shape: (1,)
+[2023-08-31 21:31:18,971][25931] On MacOS, not setting affinity
+[2023-08-31 21:31:19,005][25932] On MacOS, not setting affinity
+[2023-08-31 21:31:19,072][25929] On MacOS, not setting affinity
+[2023-08-31 21:31:19,076][25923] Created Actor Critic model with architecture:
+[2023-08-31 21:31:19,076][25923] ActorCriticSharedWeights(
+  (obs_normalizer): ObservationNormalizer(
+    (running_mean_std): RunningMeanStdDictInPlace(
+      (running_mean_std): ModuleDict(
+        (obs): RunningMeanStdInPlace()
+      )
+    )
+  )
+  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
+  (encoder): MultiInputEncoder(
+    (encoders): ModuleDict(
+      (obs): MlpEncoder(
+        (mlp_head): RecursiveScriptModule(
+          original_name=Sequential
+          (0): RecursiveScriptModule(original_name=Linear)
+          (1): RecursiveScriptModule(original_name=ELU)
+          (2): RecursiveScriptModule(original_name=Linear)
+          (3): RecursiveScriptModule(original_name=ELU)
+        )
+      )
+    )
+  )
+  (core): ModelCoreIdentity()
+  (decoder): MlpDecoder(
+    (mlp): Identity()
+  )
+  (critic_linear): Linear(in_features=64, out_features=1, bias=True)
+  (action_parameterization): ActionParameterizationDefault(
+    (distribution_linear): Linear(in_features=64, out_features=18, bias=True)
+  )
+)
+[2023-08-31 21:31:19,077][25923] Using optimizer <class 'torch.optim.adam.Adam'>
+[2023-08-31 21:31:19,078][25923] Loading state from checkpoint /Users/xinsonglin/Triped/train_dir/./example_gym_triped_sim-v0/checkpoint_p0/checkpoint_000019544_10006528.pth...
+[2023-08-31 21:31:19,080][25923] Loading model from checkpoint
+[2023-08-31 21:31:19,081][25923] Loaded experiment state at self.train_step=19544, self.env_steps=10006528
+[2023-08-31 21:31:19,081][25923] Initialized policy 0 weights for model version 19544
+[2023-08-31 21:31:19,082][25923] LearnerWorker_p0 finished initialization!
+[2023-08-31 21:31:19,085][25925] RunningMeanStd input shape: (8,)
+[2023-08-31 21:31:19,086][25925] RunningMeanStd input shape: (1,)
+[2023-08-31 21:31:19,100][25926] On MacOS, not setting affinity
+[2023-08-31 21:31:19,100][25924] On MacOS, not setting affinity
+[2023-08-31 21:31:19,100][25930] On MacOS, not setting affinity
+[2023-08-31 21:31:19,100][25927] On MacOS, not setting affinity
+[2023-08-31 21:31:19,104][25928] On MacOS, not setting affinity
+[2023-08-31 21:31:19,136][25909] Inference worker 0-0 is ready!
+[2023-08-31 21:31:19,137][25909] All inference workers are ready! Signal rollout workers to start!
+[2023-08-31 21:31:19,443][25929] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:19,446][25932] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:19,447][25931] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:19,456][25926] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:19,475][25928] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:19,495][25924] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:19,500][25930] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:19,506][25927] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:19,602][25932] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:19,624][25931] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:19,656][25929] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:19,673][25928] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:19,687][25926] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:19,687][25927] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:19,704][25930] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:19,710][25924] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:19,716][25909] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 10006528. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
+[2023-08-31 21:31:20,141][25930] Stopping RolloutWorker_w6...
+[2023-08-31 21:31:20,141][25929] Stopping RolloutWorker_w5...
+[2023-08-31 21:31:20,141][25932] Stopping RolloutWorker_w3...
+[2023-08-31 21:31:20,141][25931] Stopping RolloutWorker_w7...
+[2023-08-31 21:31:20,141][25930] Loop rollout_proc6_evt_loop terminating...
+[2023-08-31 21:31:20,141][25932] Loop rollout_proc3_evt_loop terminating...
+[2023-08-31 21:31:20,141][25931] Loop rollout_proc7_evt_loop terminating...
+[2023-08-31 21:31:20,141][25923] Stopping Batcher_0...
+[2023-08-31 21:31:20,141][25924] Stopping RolloutWorker_w1...
+[2023-08-31 21:31:20,141][25923] Loop batcher_evt_loop terminating...
+[2023-08-31 21:31:20,141][25924] Loop rollout_proc1_evt_loop terminating...
+[2023-08-31 21:31:20,141][25928] Stopping RolloutWorker_w4...
+[2023-08-31 21:31:20,142][25928] Loop rollout_proc4_evt_loop terminating...
+[2023-08-31 21:31:20,141][25929] Loop rollout_proc5_evt_loop terminating...
+[2023-08-31 21:31:20,142][25927] Stopping RolloutWorker_w2...
+[2023-08-31 21:31:20,144][25927] Loop rollout_proc2_evt_loop terminating...
+[2023-08-31 21:31:20,142][25926] Stopping RolloutWorker_w0...
+[2023-08-31 21:31:20,143][25923] Saving /Users/xinsonglin/Triped/train_dir/./example_gym_triped_sim-v0/checkpoint_p0/checkpoint_000019552_10010624.pth...
+[2023-08-31 21:31:20,145][25926] Loop rollout_proc0_evt_loop terminating...
+[2023-08-31 21:31:20,158][25923] Removing /Users/xinsonglin/Triped/train_dir/./example_gym_triped_sim-v0/checkpoint_p0/checkpoint_000017728_9076736.pth
+[2023-08-31 21:31:20,159][25923] Saving /Users/xinsonglin/Triped/train_dir/./example_gym_triped_sim-v0/checkpoint_p0/checkpoint_000019552_10010624.pth...
+[2023-08-31 21:31:20,161][25923] Stopping LearnerWorker_p0...
+[2023-08-31 21:31:20,161][25923] Loop learner_proc0_evt_loop terminating...
+[2023-08-31 21:31:20,215][25925] Weights refcount: 2 0
+[2023-08-31 21:31:20,216][25925] Stopping InferenceWorker_p0-w0...
+[2023-08-31 21:31:20,216][25925] Loop inference_proc0-0_evt_loop terminating...
+[2023-08-31 21:31:20,222][25909] Component RolloutWorker_w6 stopped!
+[2023-08-31 21:31:20,222][25909] Component RolloutWorker_w3 stopped!
+[2023-08-31 21:31:20,222][25909] Component RolloutWorker_w5 stopped!
+[2023-08-31 21:31:20,222][25909] Component RolloutWorker_w7 stopped!
+[2023-08-31 21:31:20,222][25909] Component RolloutWorker_w1 stopped!
+[2023-08-31 21:31:20,223][25909] Component Batcher_0 stopped!
+[2023-08-31 21:31:20,223][25909] Component RolloutWorker_w4 stopped!
+[2023-08-31 21:31:20,223][25909] Component RolloutWorker_w2 stopped!
+[2023-08-31 21:31:20,224][25909] Component RolloutWorker_w0 stopped!
+[2023-08-31 21:31:20,224][25909] Component LearnerWorker_p0 stopped!
+[2023-08-31 21:31:20,224][25909] Component InferenceWorker_p0-w0 stopped!
+[2023-08-31 21:31:20,224][25909] Waiting for process learner_proc0 to stop...
+[2023-08-31 21:31:20,612][25909] Waiting for process inference_proc0-0 to join...
+[2023-08-31 21:31:20,699][25909] Waiting for process rollout_proc0 to join...
+[2023-08-31 21:31:20,699][25909] Waiting for process rollout_proc1 to join...
+[2023-08-31 21:31:20,699][25909] Waiting for process rollout_proc2 to join...
+[2023-08-31 21:31:20,699][25909] Waiting for process rollout_proc3 to join...
+[2023-08-31 21:31:20,699][25909] Waiting for process rollout_proc4 to join...
+[2023-08-31 21:31:20,699][25909] Waiting for process rollout_proc5 to join...
+[2023-08-31 21:31:20,699][25909] Waiting for process rollout_proc6 to join...
+[2023-08-31 21:31:20,699][25909] Waiting for process rollout_proc7 to join...
+[2023-08-31 21:31:20,699][25909] Batcher 0 profile tree view:
+batching: 0.0016, releasing_batches: 0.0004
+[2023-08-31 21:31:20,699][25909] InferenceWorker_p0-w0 profile tree view:
+update_model: 0.0056
+wait_policy: 0.0124
+  wait_policy_total: 0.6341
+one_step: 0.0006
+  handle_policy_step: 0.3489
+    deserialize: 0.0093, stack: 0.0032, obs_to_device_normalize: 0.0608, forward: 0.1817, send_messages: 0.0383
+    prepare_outputs: 0.0300
+      to_cpu: 0.0032
+[2023-08-31 21:31:20,699][25909] Learner 0 profile tree view:
+misc: 0.0000, prepare_batch: 0.0214
+train: 0.0568
+  epoch_init: 0.0000, minibatch_init: 0.0003, losses_postprocess: 0.0010, kl_divergence: 0.0002, after_optimizer: 0.0026
+  calculate_losses: 0.0155
+    losses_init: 0.0000, forward_head: 0.0047, bptt_initial: 0.0000, bptt: 0.0001, tail: 0.0050, advantages_returns: 0.0009, losses: 0.0044
+  update: 0.0360
+    clip: 0.0029
+[2023-08-31 21:31:20,699][25909] RolloutWorker_w0 profile tree view:
+wait_for_trajectories: 0.0002, enqueue_policy_requests: 0.0135, env_step: 0.0642, overhead: 0.0123, complete_rollouts: 0.0004
+save_policy_outputs: 0.0226
+  split_output_tensors: 0.0080
+[2023-08-31 21:31:20,700][25909] RolloutWorker_w7 profile tree view:
+wait_for_trajectories: 0.0002, enqueue_policy_requests: 0.0143, env_step: 0.0679, overhead: 0.0106, complete_rollouts: 0.0007
+save_policy_outputs: 0.0222
+  split_output_tensors: 0.0076
+[2023-08-31 21:31:20,700][25909] Loop Runner_EvtLoop terminating...
+[2023-08-31 21:31:20,700][25909] Runner profile tree view:
+main_loop: 4.4786
+[2023-08-31 21:31:20,700][25909] Collected {0: 10010624}, FPS: 914.6
+[2023-08-31 21:31:45,871][26005] Saving configuration to /Users/xinsonglin/Triped/train_dir/./example_gym_triped_sim-v0/config.json...
+[2023-08-31 21:31:45,892][26005] Rollout worker 0 uses device cpu
+[2023-08-31 21:31:45,893][26005] Rollout worker 1 uses device cpu
+[2023-08-31 21:31:45,893][26005] Rollout worker 2 uses device cpu
+[2023-08-31 21:31:45,893][26005] Rollout worker 3 uses device cpu
+[2023-08-31 21:31:45,893][26005] Rollout worker 4 uses device cpu
+[2023-08-31 21:31:45,893][26005] Rollout worker 5 uses device cpu
+[2023-08-31 21:31:45,893][26005] Rollout worker 6 uses device cpu
+[2023-08-31 21:31:45,893][26005] Rollout worker 7 uses device cpu
+[2023-08-31 21:31:45,893][26005] In synchronous mode, we only accumulate one batch. Setting num_batches_to_accumulate to 1
+[2023-08-31 21:31:46,077][26005] InferenceWorker_p0-w0: min num requests: 2
+[2023-08-31 21:31:46,122][26005] Starting all processes...
+[2023-08-31 21:31:46,122][26005] Starting process learner_proc0
+[2023-08-31 21:31:46,175][26005] Starting all processes...
+[2023-08-31 21:31:46,207][26005] Starting process inference_proc0-0
+[2023-08-31 21:31:46,215][26005] Starting process rollout_proc0
+[2023-08-31 21:31:46,231][26005] Starting process rollout_proc1
+[2023-08-31 21:31:46,234][26005] Starting process rollout_proc2
+[2023-08-31 21:31:46,250][26005] Starting process rollout_proc3
+[2023-08-31 21:31:46,261][26005] Starting process rollout_proc4
+[2023-08-31 21:31:46,267][26005] Starting process rollout_proc5
+[2023-08-31 21:31:46,270][26005] Starting process rollout_proc6
+[2023-08-31 21:31:46,276][26005] Starting process rollout_proc7
+[2023-08-31 21:31:48,626][26019] On MacOS, not setting affinity
+[2023-08-31 21:31:48,706][26018] On MacOS, not setting affinity
+[2023-08-31 21:31:48,733][26016] Starting seed is not provided
+[2023-08-31 21:31:48,733][26016] Initializing actor-critic model on device cpu
+[2023-08-31 21:31:48,734][26016] RunningMeanStd input shape: (8,)
+[2023-08-31 21:31:48,735][26016] RunningMeanStd input shape: (1,)
+[2023-08-31 21:31:48,758][26020] On MacOS, not setting affinity
+[2023-08-31 21:31:48,758][26025] On MacOS, not setting affinity
+[2023-08-31 21:31:48,798][26022] On MacOS, not setting affinity
+[2023-08-31 21:31:48,798][26021] On MacOS, not setting affinity
+[2023-08-31 21:31:48,805][26023] On MacOS, not setting affinity
+[2023-08-31 21:31:48,841][26016] Created Actor Critic model with architecture:
+[2023-08-31 21:31:48,841][26016] ActorCriticSharedWeights(
+  (obs_normalizer): ObservationNormalizer(
+    (running_mean_std): RunningMeanStdDictInPlace(
+      (running_mean_std): ModuleDict(
+        (obs): RunningMeanStdInPlace()
+      )
+    )
+  )
+  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
+  (encoder): MultiInputEncoder(
+    (encoders): ModuleDict(
+      (obs): MlpEncoder(
+        (mlp_head): RecursiveScriptModule(
+          original_name=Sequential
+          (0): RecursiveScriptModule(original_name=Linear)
+          (1): RecursiveScriptModule(original_name=ELU)
+          (2): RecursiveScriptModule(original_name=Linear)
+          (3): RecursiveScriptModule(original_name=ELU)
+        )
+      )
+    )
+  )
+  (core): ModelCoreIdentity()
+  (decoder): MlpDecoder(
+    (mlp): Identity()
+  )
+  (critic_linear): Linear(in_features=64, out_features=1, bias=True)
+  (action_parameterization): ActionParameterizationDefault(
+    (distribution_linear): Linear(in_features=64, out_features=18, bias=True)
+  )
+)
+[2023-08-31 21:31:48,847][26016] Using optimizer <class 'torch.optim.adam.Adam'>
+[2023-08-31 21:31:48,850][26016] Loading state from checkpoint /Users/xinsonglin/Triped/train_dir/./example_gym_triped_sim-v0/checkpoint_p0/checkpoint_000019552_10010624.pth...
+[2023-08-31 21:31:48,850][26024] On MacOS, not setting affinity
+[2023-08-31 21:31:48,852][26016] Loading model from checkpoint
+[2023-08-31 21:31:48,854][26016] Loaded experiment state at self.train_step=19552, self.env_steps=10010624
+[2023-08-31 21:31:48,854][26016] Initialized policy 0 weights for model version 19552
+[2023-08-31 21:31:48,855][26016] LearnerWorker_p0 finished initialization!
+[2023-08-31 21:31:48,856][26017] RunningMeanStd input shape: (8,)
+[2023-08-31 21:31:48,859][26017] RunningMeanStd input shape: (1,)
+[2023-08-31 21:31:48,908][26005] Inference worker 0-0 is ready!
+[2023-08-31 21:31:48,908][26005] All inference workers are ready! Signal rollout workers to start!
+[2023-08-31 21:31:49,134][26023] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:49,134][26020] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:49,147][26021] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:49,148][26018] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:49,149][26019] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:49,151][26025] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:49,171][26022] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:49,172][26024] Decorrelating experience for 0 frames...
+[2023-08-31 21:31:49,300][26020] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:49,304][26019] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:49,315][26018] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:49,315][26025] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:49,323][26023] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:49,325][26022] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:49,327][26021] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:49,336][26024] Decorrelating experience for 64 frames...
+[2023-08-31 21:31:49,737][26005] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 10014720. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:31:52,075][26017] Updated weights for policy 0, policy_version 19632 (0.0003)
+[2023-08-31 21:31:54,628][26017] Updated weights for policy 0, policy_version 19712 (0.0003)
+[2023-08-31 21:31:54,735][26005] Fps is (10 sec: 15570.4, 60 sec: 15570.4, 300 sec: 15570.4). Total num frames: 10092544. Throughput: 0: 7833.2. Samples: 39152. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:31:57,294][26017] Updated weights for policy 0, policy_version 19792 (0.0003)
+[2023-08-31 21:31:59,736][26005] Fps is (10 sec: 14747.1, 60 sec: 14747.1, 300 sec: 14747.1). Total num frames: 10162176. Throughput: 0: 13110.2. Samples: 131088. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:32:00,676][26017] Updated weights for policy 0, policy_version 19872 (0.0003)
+[2023-08-31 21:32:03,546][26017] Updated weights for policy 0, policy_version 19952 (0.0003)
+[2023-08-31 21:32:04,737][26005] Fps is (10 sec: 13924.9, 60 sec: 14473.2, 300 sec: 14473.2). Total num frames: 10231808. Throughput: 0: 14066.9. Samples: 210993. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
+[2023-08-31 21:32:04,737][26005] Avg episode reward: [(0, '-10316.262')]
+[2023-08-31 21:32:04,738][26016] Saving new best policy, reward=-10316.262!
+[2023-08-31 21:32:06,065][26005] Heartbeat connected on Batcher_0
+[2023-08-31 21:32:06,067][26005] Heartbeat connected on LearnerWorker_p0
+[2023-08-31 21:32:06,077][26005] Heartbeat connected on InferenceWorker_p0-w0
+[2023-08-31 21:32:06,082][26005] Heartbeat connected on RolloutWorker_w0
+[2023-08-31 21:32:06,087][26005] Heartbeat connected on RolloutWorker_w1
+[2023-08-31 21:32:06,092][26005] Heartbeat connected on RolloutWorker_w2
+[2023-08-31 21:32:06,097][26005] Heartbeat connected on RolloutWorker_w3
+[2023-08-31 21:32:06,100][26005] Heartbeat connected on RolloutWorker_w4
+[2023-08-31 21:32:06,105][26005] Heartbeat connected on RolloutWorker_w5
+[2023-08-31 21:32:06,118][26005] Heartbeat connected on RolloutWorker_w6
+[2023-08-31 21:32:06,122][26005] Heartbeat connected on RolloutWorker_w7
+[2023-08-31 21:32:06,237][26017] Updated weights for policy 0, policy_version 20032 (0.0003)
+[2023-08-31 21:32:07,503][26016] KL-divergence is very high: 125.1986
+[2023-08-31 21:32:08,019][26016] KL-divergence is very high: 193.2574
+[2023-08-31 21:32:08,815][26017] Updated weights for policy 0, policy_version 20112 (0.0003)
+[2023-08-31 21:32:09,735][26005] Fps is (10 sec: 14748.0, 60 sec: 14747.6, 300 sec: 14747.6). Total num frames: 10309632. Throughput: 0: 12862.5. Samples: 257215. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
+[2023-08-31 21:32:09,735][26005] Avg episode reward: [(0, '-10316.262')]
+[2023-08-31 21:32:12,132][26017] Updated weights for policy 0, policy_version 20192 (0.0003)
+[2023-08-31 21:32:12,406][26016] KL-divergence is very high: 136.7137
+[2023-08-31 21:32:14,735][26005] Fps is (10 sec: 14338.2, 60 sec: 14419.2, 300 sec: 14419.2). Total num frames: 10375168. Throughput: 0: 13600.6. Samples: 339984. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
+[2023-08-31 21:32:14,735][26005] Avg episode reward: [(0, '-11568.545')]
+[2023-08-31 21:32:14,765][26017] Updated weights for policy 0, policy_version 20272 (0.0003)
+[2023-08-31 21:32:17,331][26017] Updated weights for policy 0, policy_version 20352 (0.0002)
+[2023-08-31 21:32:19,712][26016] KL-divergence is very high: 121.9458
+[2023-08-31 21:32:19,714][26017] Updated weights for policy 0, policy_version 20432 (0.0002)
+[2023-08-31 21:32:19,737][26005] Fps is (10 sec: 15151.9, 60 sec: 14882.4, 300 sec: 14882.4). Total num frames: 10461184. Throughput: 0: 14579.1. Samples: 437365. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:32:19,737][26005] Avg episode reward: [(0, '-11568.545')]
+[2023-08-31 21:32:22,837][26017] Updated weights for policy 0, policy_version 20512 (0.0003)
+[2023-08-31 21:32:24,735][26005] Fps is (10 sec: 15155.5, 60 sec: 14629.6, 300 sec: 14629.6). Total num frames: 10526720. Throughput: 0: 13719.6. Samples: 480152. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:32:24,735][26005] Avg episode reward: [(0, '-9213.608')]
+[2023-08-31 21:32:24,735][26016] Saving new best policy, reward=-9213.608!
+[2023-08-31 21:32:25,484][26017] Updated weights for policy 0, policy_version 20592 (0.0003)
+[2023-08-31 21:32:28,100][26017] Updated weights for policy 0, policy_version 20672 (0.0003)
+[2023-08-31 21:32:29,737][26005] Fps is (10 sec: 14745.2, 60 sec: 14848.1, 300 sec: 14848.1). Total num frames: 10608640. Throughput: 0: 14281.2. Samples: 571246. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:32:29,737][26005] Avg episode reward: [(0, '-9213.608')]
+[2023-08-31 21:32:30,688][26017] Updated weights for policy 0, policy_version 20752 (0.0002)
+[2023-08-31 21:32:33,494][26017] Updated weights for policy 0, policy_version 20832 (0.0002)
+[2023-08-31 21:32:34,511][26016] KL-divergence is very high: 137.8809
+[2023-08-31 21:32:34,737][26005] Fps is (10 sec: 15561.8, 60 sec: 14836.8, 300 sec: 14836.8). Total num frames: 10682368. Throughput: 0: 14705.1. Samples: 661722. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:32:34,737][26005] Avg episode reward: [(0, '-8001.997')]
+[2023-08-31 21:32:34,738][26016] Saving new best policy, reward=-8001.997!
+[2023-08-31 21:32:36,079][26017] Updated weights for policy 0, policy_version 20912 (0.0003)
+[2023-08-31 21:32:38,508][26017] Updated weights for policy 0, policy_version 20992 (0.0002)
+[2023-08-31 21:32:39,736][26005] Fps is (10 sec: 15566.8, 60 sec: 14991.8, 300 sec: 14991.8). Total num frames: 10764288. Throughput: 0: 14912.5. Samples: 710220. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:32:39,736][26005] Avg episode reward: [(0, '-8001.997')]
+[2023-08-31 21:32:41,044][26017] Updated weights for policy 0, policy_version 21072 (0.0002)
+[2023-08-31 21:32:43,840][26017] Updated weights for policy 0, policy_version 21152 (0.0002)
+[2023-08-31 21:32:44,737][26005] Fps is (10 sec: 15974.3, 60 sec: 15043.6, 300 sec: 15043.6). Total num frames: 10842112. Throughput: 0: 14952.0. Samples: 803937. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:32:44,737][26005] Avg episode reward: [(0, '-7977.126')]
+[2023-08-31 21:32:44,737][26016] Saving new best policy, reward=-7977.126!
+[2023-08-31 21:32:46,346][26017] Updated weights for policy 0, policy_version 21232 (0.0003)
+[2023-08-31 21:32:48,888][26017] Updated weights for policy 0, policy_version 21312 (0.0003)
+[2023-08-31 21:32:49,736][26005] Fps is (10 sec: 15974.0, 60 sec: 15155.5, 300 sec: 15155.5). Total num frames: 10924032. Throughput: 0: 15344.0. Samples: 901465. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:32:49,736][26005] Avg episode reward: [(0, '-7977.126')]
+[2023-08-31 21:32:51,498][26017] Updated weights for policy 0, policy_version 21392 (0.0003)
+[2023-08-31 21:32:54,423][26017] Updated weights for policy 0, policy_version 21472 (0.0003)
+[2023-08-31 21:32:54,736][26005] Fps is (10 sec: 15565.2, 60 sec: 15086.7, 300 sec: 15123.9). Total num frames: 10997760. Throughput: 0: 15379.5. Samples: 949322. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:32:54,737][26005] Avg episode reward: [(0, '-7017.920')]
+[2023-08-31 21:32:54,737][26016] Saving new best policy, reward=-7017.920!
+[2023-08-31 21:32:57,062][26017] Updated weights for policy 0, policy_version 21552 (0.0003)
+[2023-08-31 21:32:59,735][26005] Fps is (10 sec: 14336.8, 60 sec: 15087.1, 300 sec: 15038.6). Total num frames: 11067392. Throughput: 0: 15507.3. Samples: 1037819. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:32:59,736][26005] Avg episode reward: [(0, '-7017.920')]
+[2023-08-31 21:33:00,179][26017] Updated weights for policy 0, policy_version 21632 (0.0003)
+[2023-08-31 21:33:03,208][26017] Updated weights for policy 0, policy_version 21712 (0.0003)
+[2023-08-31 21:33:04,736][26005] Fps is (10 sec: 13107.4, 60 sec: 14950.5, 300 sec: 14855.0). Total num frames: 11128832. Throughput: 0: 15043.8. Samples: 1114328. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:33:04,737][26005] Avg episode reward: [(0, '-6334.574')]
+[2023-08-31 21:33:04,755][26016] Saving new best policy, reward=-6334.574!
+[2023-08-31 21:33:06,547][26017] Updated weights for policy 0, policy_version 21792 (0.0003)
+[2023-08-31 21:33:09,474][26017] Updated weights for policy 0, policy_version 21872 (0.0003)
+[2023-08-31 21:33:09,737][26005] Fps is (10 sec: 13514.9, 60 sec: 14881.6, 300 sec: 14848.1). Total num frames: 11202560. Throughput: 0: 14908.2. Samples: 1151054. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:33:09,737][26005] Avg episode reward: [(0, '-6334.574')]
+[2023-08-31 21:33:12,146][26017] Updated weights for policy 0, policy_version 21952 (0.0003)
+[2023-08-31 21:33:14,736][26005] Fps is (10 sec: 14745.6, 60 sec: 15018.3, 300 sec: 14842.1). Total num frames: 11276288. Throughput: 0: 14870.0. Samples: 1240384. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:33:14,736][26005] Avg episode reward: [(0, '-6334.574')]
+[2023-08-31 21:33:14,972][26017] Updated weights for policy 0, policy_version 22032 (0.0002)
+[2023-08-31 21:33:18,047][26017] Updated weights for policy 0, policy_version 22112 (0.0003)
+[2023-08-31 21:33:19,735][26005] Fps is (10 sec: 14338.5, 60 sec: 14746.0, 300 sec: 14791.5). Total num frames: 11345920. Throughput: 0: 14703.4. Samples: 1323353. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:33:19,735][26005] Avg episode reward: [(0, '-4526.619')]
+[2023-08-31 21:33:19,736][26016] Saving new best policy, reward=-4526.619!
+[2023-08-31 21:33:20,766][26017] Updated weights for policy 0, policy_version 22192 (0.0003)
+[2023-08-31 21:33:23,401][26017] Updated weights for policy 0, policy_version 22272 (0.0003)
+[2023-08-31 21:33:24,735][26005] Fps is (10 sec: 14338.0, 60 sec: 14882.1, 300 sec: 14789.1). Total num frames: 11419648. Throughput: 0: 14665.7. Samples: 1370165. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:33:24,735][26005] Avg episode reward: [(0, '-4526.619')]
+[2023-08-31 21:33:24,738][26016] Saving /Users/xinsonglin/Triped/train_dir/./example_gym_triped_sim-v0/checkpoint_p0/checkpoint_000022304_11419648.pth...
+[2023-08-31 21:33:24,740][26016] Removing /Users/xinsonglin/Triped/train_dir/./example_gym_triped_sim-v0/checkpoint_p0/checkpoint_000019544_10006528.pth
+[2023-08-31 21:33:26,371][26017] Updated weights for policy 0, policy_version 22352 (0.0003)
+[2023-08-31 21:33:29,425][26017] Updated weights for policy 0, policy_version 22432 (0.0003)
+[2023-08-31 21:33:29,737][26005] Fps is (10 sec: 14333.7, 60 sec: 14677.4, 300 sec: 14745.7). Total num frames: 11489280. Throughput: 0: 14381.3. Samples: 1451096. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
+[2023-08-31 21:33:29,737][26005] Avg episode reward: [(0, '-3991.432')]
+[2023-08-31 21:33:29,737][26016] Saving new best policy, reward=-3991.432!
+[2023-08-31 21:33:32,106][26017] Updated weights for policy 0, policy_version 22512 (0.0003)
+[2023-08-31 21:33:34,736][26005] Fps is (10 sec: 13925.1, 60 sec: 14609.3, 300 sec: 14706.8). Total num frames: 11558912. Throughput: 0: 14191.1. Samples: 1540059. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
+[2023-08-31 21:33:34,736][26005] Avg episode reward: [(0, '-3991.432')]
+[2023-08-31 21:33:35,122][26017] Updated weights for policy 0, policy_version 22592 (0.0003)
+[2023-08-31 21:33:38,084][26017] Updated weights for policy 0, policy_version 22672 (0.0003)
+[2023-08-31 21:33:39,735][26005] Fps is (10 sec: 13109.4, 60 sec: 14267.9, 300 sec: 14596.9). Total num frames: 11620352. Throughput: 0: 14068.1. Samples: 1582366. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
+[2023-08-31 21:33:39,736][26005] Avg episode reward: [(0, '-4414.125')]
+[2023-08-31 21:33:42,030][26017] Updated weights for policy 0, policy_version 22752 (0.0003)
+[2023-08-31 21:33:44,740][26005] Fps is (10 sec: 12691.7, 60 sec: 14062.1, 300 sec: 14531.5). Total num frames: 11685888. Throughput: 0: 13618.3. Samples: 1650708. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
+[2023-08-31 21:33:44,741][26005] Avg episode reward: [(0, '-4414.125')]
+[2023-08-31 21:33:44,899][26017] Updated weights for policy 0, policy_version 22832 (0.0002)
+[2023-08-31 21:33:47,630][26017] Updated weights for policy 0, policy_version 22912 (0.0003)
+[2023-08-31 21:33:49,736][26005] Fps is (10 sec: 14334.2, 60 sec: 13994.6, 300 sec: 14575.0). Total num frames: 11763712. Throughput: 0: 13922.1. Samples: 1740821. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
+[2023-08-31 21:33:49,736][26005] Avg episode reward: [(0, '-4414.125')]
+[2023-08-31 21:33:50,549][26017] Updated weights for policy 0, policy_version 22992 (0.0002)
+[2023-08-31 21:33:53,099][26017] Updated weights for policy 0, policy_version 23072 (0.0002)
+[2023-08-31 21:33:54,735][26005] Fps is (10 sec: 14753.6, 60 sec: 13926.7, 300 sec: 14549.3). Total num frames: 11833344. Throughput: 0: 14036.7. Samples: 1782679. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
+[2023-08-31 21:33:54,735][26005] Avg episode reward: [(0, '-3530.495')]
+[2023-08-31 21:33:54,762][26016] Saving new best policy, reward=-3530.495!
+[2023-08-31 21:33:56,241][26017] Updated weights for policy 0, policy_version 23152 (0.0003)
+[2023-08-31 21:33:58,951][26017] Updated weights for policy 0, policy_version 23232 (0.0003)
+[2023-08-31 21:33:59,737][26005] Fps is (10 sec: 13926.0, 60 sec: 13926.1, 300 sec: 14525.1). Total num frames: 11902976. Throughput: 0: 13942.3. Samples: 1867794. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
+[2023-08-31 21:33:59,737][26005] Avg episode reward: [(0, '-3530.495')]
+[2023-08-31 21:34:00,527][26005] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 26005], exiting...
+[2023-08-31 21:34:00,527][26005] Runner profile tree view:
+main_loop: 134.4053
+[2023-08-31 21:34:00,527][26005] Collected {0: 11915264}, FPS: 14170.9
+[2023-08-31 21:34:00,528][26021] Stopping RolloutWorker_w3...
+[2023-08-31 21:34:00,528][26019] Stopping RolloutWorker_w1...
+[2023-08-31 21:34:00,528][26024] Stopping RolloutWorker_w6...
+[2023-08-31 21:34:00,528][26023] Stopping RolloutWorker_w5...
+[2023-08-31 21:34:00,528][26022] Stopping RolloutWorker_w4...
+[2023-08-31 21:34:00,528][26019] Loop rollout_proc1_evt_loop terminating...
+[2023-08-31 21:34:00,528][26021] Loop rollout_proc3_evt_loop terminating...
+[2023-08-31 21:34:00,528][26023] Loop rollout_proc5_evt_loop terminating...
+[2023-08-31 21:34:00,528][26022] Loop rollout_proc4_evt_loop terminating...
+[2023-08-31 21:34:00,528][26024] Loop rollout_proc6_evt_loop terminating...
+[2023-08-31 21:34:00,529][26016] Stopping Batcher_0...
+[2023-08-31 21:34:00,531][26016] Loop batcher_evt_loop terminating...
+[2023-08-31 21:34:00,529][26018] Stopping RolloutWorker_w0...
+[2023-08-31 21:34:00,528][26020] Stopping RolloutWorker_w2...
+[2023-08-31 21:34:00,531][26025] Stopping RolloutWorker_w7...
+[2023-08-31 21:34:00,532][26025] Loop rollout_proc7_evt_loop terminating...
+[2023-08-31 21:34:00,530][26016] Saving /Users/xinsonglin/Triped/train_dir/./example_gym_triped_sim-v0/checkpoint_p0/checkpoint_000023272_11915264.pth...
+[2023-08-31 21:34:00,532][26020] Loop rollout_proc2_evt_loop terminating...
+[2023-08-31 21:34:00,533][26018] Loop rollout_proc0_evt_loop terminating...
+[2023-08-31 21:34:00,534][26016] Removing /Users/xinsonglin/Triped/train_dir/./example_gym_triped_sim-v0/checkpoint_p0/checkpoint_000019552_10010624.pth
+[2023-08-31 21:34:00,537][26016] Stopping LearnerWorker_p0...
+[2023-08-31 21:34:00,538][26016] Loop learner_proc0_evt_loop terminating...
+[2023-08-31 21:34:00,703][26017] Weights refcount: 2 0
+[2023-08-31 21:34:00,704][26017] Stopping InferenceWorker_p0-w0...
+[2023-08-31 21:34:00,704][26017] Loop inference_proc0-0_evt_loop terminating...
+[2023-08-31 21:34:04,174][26276] Saving configuration to /Users/xinsonglin/Triped/train_dir/./example_gym_triped_sim-v0/config.json...
